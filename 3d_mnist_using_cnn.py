# -*- coding: utf-8 -*-
"""3D MNIST using CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12NBMwxnGTHTd_UqhTp93jqpNc7I_ZNRs

In 2 dimentional image Segmentation we use 2d convolution network here in this project we are trying to use a 3d CNN model for classification.For classifying 3d datasets pointnets are better alternative but it requires creating complex architecture.MNIST dataset consists of digits from 0-9.
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
os.environ['KAGGLE_CONFIG_DIR'] = "/content/gdrive/My Drive/Kaggle"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/My Drive/Kaggle

"""First we are downloading the dataset from kaggle."""

!kaggle datasets download -d daavoo/3d-mnist

!unzip \*.zip  && rm *.zip

import numpy as np
import h5py

"""Then we are seperating training and testing data from the dataset."""

with h5py.File('full_dataset_vectors.h5', 'r') as dataset:
    x_train = dataset["X_train"][:]
    x_test = dataset["X_test"][:]
    y_train = dataset["y_train"][:]
    y_test = dataset["y_test"][:]

print ("x_train shape: ", x_train.shape)
print ("y_train shape: ", y_train.shape)

print ("x_test shape:  ", x_test.shape)
print ("y_test shape:  ", y_test.shape)

"""We add 3 color channels to data data similar to as we do 3 channels to image set"""

xtrain = np.ndarray((x_train.shape[0], 4096, 3))
xtest = np.ndarray((x_test.shape[0], 4096, 3))

from matplotlib.pyplot import cm

def add_rgb_dimention(array):
    scalar_map = cm.ScalarMappable(cmap="Oranges")
    return scalar_map.to_rgba(array)[:, : -1]

for i in range(x_train.shape[0]):
    xtrain[i] = add_rgb_dimention(x_train[i])
for i in range(x_test.shape[0]):
    xtest[i] = add_rgb_dimention(x_test[i])

xtrain = xtrain.reshape(x_train.shape[0], 16, 16, 16, 3)
xtest = xtest.reshape(x_test.shape[0], 16, 16, 16, 3)

xtrain.shape

"""Now set the lables to one-hot matrix for classification."""

import keras.utils

y_train = keras.utils.to_categorical(y_train,10)
y_test = keras.utils.to_categorical(y_test,10)

y_train.shape

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPool3D, Flatten, Dense
from tensorflow.keras.layers import Dropout,BatchNormalization
model=Sequential()
model.add(Conv3D(32,(3,3,3),activation='relu',padding='same',input_shape=xtrain.shape[1:]))
model.add(MaxPool3D((2,2,2)))
model.add(Conv3D(64,(3,3,3),activation='relu',padding='same'))
model.add(MaxPool3D((2,2,2)))
model.add(Conv3D(128,(3,3,3),activation='relu',padding='same'))
model.add(MaxPool3D((2,2,2)))
model.add(Conv3D(256,(3,3,3),activation='relu',padding='same'))
model.add(MaxPool3D((2,2,2)))

model.summary()

model.add(Flatten())
model.add(Dense(128,activation="relu"))
model.add(Dropout(0.5))
model.add(Dense(10,activation="softmax"))

model.summary()

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history=model.fit(x=xtrain, y=y_train, batch_size=128, epochs=50, validation_split=0.15)

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

model.save('model2.h5')
from google.colab import files
files.download("model2.h5")